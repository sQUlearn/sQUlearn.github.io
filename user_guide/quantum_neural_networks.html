<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Quantum Neural Networks &mdash; sQUlearn 0.4.0 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/plot_directive.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js?v=4e78f113"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=4825356b"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="API Reference" href="../modules/classes.html" />
    <link rel="prev" title="Quantum Kernel Methods" href="kernel_methods.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            sQUlearn
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../install/install.html">Installation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="user_guide_index.html">User Guide</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="executor.html">The Executor Class</a></li>
<li class="toctree-l2"><a class="reference internal" href="observables.html">Observables for expectation values</a></li>
<li class="toctree-l2"><a class="reference internal" href="encoding_circuits.html">Quantum Encoding Circuits</a></li>
<li class="toctree-l2"><a class="reference internal" href="kernel_methods.html">Quantum Kernel Methods</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Quantum Neural Networks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#high-level-methods-for-qnns">High-level methods for QNNs</a></li>
<li class="toctree-l3"><a class="reference internal" href="#optimization">Optimization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#slsqp">SLSQP</a></li>
<li class="toctree-l4"><a class="reference internal" href="#mini-batch-gradient-descent-with-adam">Mini-Batch gradient descent with Adam</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#variance-reduction">Variance reduction</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../modules/classes.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/examples_index.html">Examples</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">sQUlearn</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="user_guide_index.html">User Guide</a></li>
      <li class="breadcrumb-item active">Quantum Neural Networks</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/user_guide/quantum_neural_networks.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="quantum-neural-networks">
<span id="id1"></span><h1>Quantum Neural Networks<a class="headerlink" href="#quantum-neural-networks" title="Permalink to this heading"></a></h1>
<p>Quantum Neural Networks (QNNs) extend the concept of artificial neural networks into the realm of
quantum computing. Typically, they are constructed by encoding input data into a quantum state
through a sequence of quantum gates. This quantum state is then manipulated using trainable
parameters and utilized to evaluate an expectation value of an observable that acts as the output
of the QNN. This output is then used to calculate a loss function, which is subsequently
minimized by a classical optimizer. The resultant QNN can then be employed to predict outcomes
for new input data.</p>
<p>In many cases, QNNs adhere to a layered design, akin to classical neural networks, as illustrated
in <a class="reference internal" href="#figure-1">figure 1</a>. However, it is essential to note that they do not adhere to the concept of
neurons as seen in classical neural networks. Therefore, the term “Quantum Neural Network” may
be somewhat misleading, as QNNs do not conform to the traditional neural network paradigm.
Nevertheless, their application domain closely resembles that of classical neural networks,
which explains the established nomenclature.</p>
<figure class="align-center" id="id3">
<span id="figure-1"></span><a class="reference internal image-reference" href="../_images/qnn.svg"><img alt="Quantum Neural Network (QNN)" src="../_images/qnn.svg" width="600" /></a>
<figcaption>
<p><span class="caption-text"><strong>Figure 1</strong> Layered design of a QNN with alternating encoding (orange) and parameter (blue) layers.
The QNN is trained in a hybrid quantum-classical scheme by optimizing the QNN’s parameters
<span class="math notranslate nohighlight">\({\theta}\)</span> for a given cost function <span class="math notranslate nohighlight">\(L\)</span>.</span><a class="headerlink" href="#id3" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>In principle, the design of QNN architectures offers a high degree of freedom.
Nevertheless, most common designs follow a layered structure, where each layer comprises an
encoding layer denoted as <span class="math notranslate nohighlight">\(U_i({x})\)</span> and a parameterized layer represented as
<span class="math notranslate nohighlight">\(U_i({\theta})\)</span>. The encoding layers map the input data, <span class="math notranslate nohighlight">\({x}\)</span>, to a quantum state of
the qubits, while the parameterized layers are tailored to modify the mapped state.</p>
<p>The selection of the encoding method depends on the specific problem and the characteristics of
the input data, whereas parameterized layers are explicitly designed to alter the mapped state.
Furthermore, entanglement among the qubits is introduced, enabling the QNN to process information
in a more intricate and interconnected manner. Finally, we repeatedly measure the resulting state,
denoted as <span class="math notranslate nohighlight">\(\Psi({x}, {\theta})\)</span>, to evaluate the QNN’s output as the expectation value:</p>
<div class="math notranslate nohighlight">
\[f({x}, {\theta}) = \langle\Psi({x}, {\theta}) \lvert\hat{C}({\theta})
\rvert\Psi({x}, {\theta}) \rangle\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\hat{C}({\theta})\)</span> represents a operator, also called observable, for each output
of the QNN. While the observable can be freely selected, it often involves operators based on a
specific type of Pauli matrices, such as the Pauli Z matrix, to simplify the
evaluation of the expectation.</p>
<p>It’s worth noting that both the embedding layers <span class="math notranslate nohighlight">\(U_i({x})\)</span> and the observable
<span class="math notranslate nohighlight">\(\hat{C}\)</span> may also contain additional trainable parameters.</p>
<p>To train Quantum Neural Networks (QNNs), a hybrid quantum-classical approach is employed.
The training process consists of two phases: quantum circuit evaluation and classical optimization
(as illustrated in <a class="reference internal" href="#figure-1">figure 1</a>).</p>
<p>In the quantum circuit evaluation phase, the QNN and its gradient with respect to the parameters
are assessed using a quantum computer or simulator. The gradient can be obtained using the
parameter-shift rule. Subsequently, in the classical optimization phase, an appropriate
classical optimization algorithm is employed to update the QNN’s parameters.
This iterative process is repeated until the desired level of accuracy is attained.</p>
<p>Commonly used classical optimizers, such as SLSQP (for simulators) or stochastic gradient
descent, like Adam, are applied in the classical optimization stage of QNN training.
They adjust the QNN’s parameters to minimize a predefined cost function, denoted as <span class="math notranslate nohighlight">\(L\)</span>:</p>
<div class="math notranslate nohighlight">
\[\min_{\theta} L(f, {x}, {\theta})\]</div>
<p>The specific form of the cost function depends on the problem that the QNN is designed to solve.
For instance, in a regression problem, the cost function is often defined as the mean squared
error between the QNN’s output and the target value.</p>
<section id="high-level-methods-for-qnns">
<h2>High-level methods for QNNs<a class="headerlink" href="#high-level-methods-for-qnns" title="Permalink to this heading"></a></h2>
<p>In this section, we will illustrate the process of constructing a QNN using sQUlearn.
A QNN is composed of two main components: an encoding circuit, which is essentially a
parameterized quantum circuit, and a cost operator.</p>
<p>In sQUlearn, we have dedicated classes for these components: <code class="xref py py-class docutils literal notranslate"><span class="pre">EncodingCircuit</span></code>
and <code class="xref py py-class docutils literal notranslate"><span class="pre">CostOperator</span></code>, both of which we will utilize in the upcoming example.</p>
<p>In the following cell, we will build an encoding circuit based on the Chebyshev input encoding
method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">squlearn.encoding_circuit</span> <span class="kn">import</span> <span class="n">ChebPQC</span>

<span class="n">pqc</span> <span class="o">=</span> <span class="n">ChebPQC</span><span class="p">(</span><span class="n">num_qubits</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">num_features</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">num_layers</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">pqc</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="s2">&quot;mpl&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>(<a class="reference download internal" download="" href="../_downloads/c05c3699672af090eea3c622fa0636b6/quantum_neural_networks-1.py"><code class="xref download docutils literal notranslate"><span class="pre">Source</span> <span class="pre">code</span></code></a>, <a class="reference download internal" download="" href="../_downloads/bf22250e911bc38fc607928e3c24d770/quantum_neural_networks-1.png"><code class="xref download docutils literal notranslate"><span class="pre">png</span></code></a>, <a class="reference download internal" download="" href="../_downloads/6742fed3dd99a4e29057a7711c1c95b2/quantum_neural_networks-1.hires.png"><code class="xref download docutils literal notranslate"><span class="pre">hires.png</span></code></a>, <a class="reference download internal" download="" href="../_downloads/354a12b3c12b6e1970f17719ee063e28/quantum_neural_networks-1.pdf"><code class="xref download docutils literal notranslate"><span class="pre">pdf</span></code></a>)</p>
<figure class="align-default">
<img alt="../_images/quantum_neural_networks-1.png" class="plot-directive" src="../_images/quantum_neural_networks-1.png" />
</figure>
<p>There are several alternative encoding circuits at your disposal in sQUlearn, which you can
explore in the user guide located at <a class="reference internal" href="encoding_circuits.html#quantum-encoding-circuits"><span class="std std-ref">Quantum Encoding Circuits</span></a>.</p>
<p>The second ingredient is to specify an observable for computing the QNN’s output. In this
particular example, we employ a summation over a Pauli Z observable for each qubit,
along with a constant offset:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">squlearn.observables</span> <span class="kn">import</span> <span class="n">SummedPaulis</span>

<span class="n">op</span> <span class="o">=</span> <span class="n">SummedPaulis</span><span class="p">(</span><span class="n">num_qubits</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<p>Other expectation operators can be found in the user guide on <a class="reference internal" href="observables.html#user-guide-observables"><span class="std std-ref">Observables for expectation values</span></a>.</p>
<p>Now we can construct a QNN from the encoding circuit and the cost operator.
sQUlearn offers two easy-to-use implementation of QNNs, either for regression or classification:</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="../modules/generated/squlearn.qnn.QNNClassifier.html#squlearn.qnn.QNNClassifier" title="squlearn.qnn.QNNClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QNNClassifier</span></code></a></p></td>
<td><p>Quantum Neural Network for Classification.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="../modules/generated/squlearn.qnn.QNNRegressor.html#squlearn.qnn.QNNRegressor" title="squlearn.qnn.QNNRegressor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QNNRegressor</span></code></a></p></td>
<td><p>Quantum Neural Network for Regression.</p></td>
</tr>
</tbody>
</table>
<p>We refer to the documentations and examples of the respective classes for in-depth information.</p>
<p>In the following example we will use a <a class="reference internal" href="../modules/generated/squlearn.qnn.QNNRegressor.html#squlearn.qnn.QNNRegressor" title="squlearn.qnn.QNNRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">QNNRegressor</span></code></a>, the encoding circuit, and
the observable as defined above. Additionally, we utilize the mean squared error loss function
and the Adam optimizer for optimization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">squlearn.observables</span> <span class="kn">import</span> <span class="n">SummedPaulis</span>
<span class="kn">from</span> <span class="nn">squlearn.encoding_circuit</span> <span class="kn">import</span> <span class="n">ChebPQC</span>
<span class="kn">from</span> <span class="nn">squlearn.qnn</span> <span class="kn">import</span> <span class="n">QNNRegressor</span><span class="p">,</span> <span class="n">SquaredLoss</span>
<span class="kn">from</span> <span class="nn">squlearn.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="nn">squlearn</span> <span class="kn">import</span> <span class="n">Executor</span>

<span class="n">op</span> <span class="o">=</span> <span class="n">SummedPaulis</span><span class="p">(</span><span class="n">num_qubits</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">pqc</span> <span class="o">=</span> <span class="n">ChebPQC</span><span class="p">(</span><span class="n">num_qubits</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">num_features</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">num_layers</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">qnn</span> <span class="o">=</span> <span class="n">QNNRegressor</span><span class="p">(</span><span class="n">pqc</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">Executor</span><span class="p">(</span><span class="s2">&quot;statevector_simulator&quot;</span><span class="p">),</span> <span class="n">SquaredLoss</span><span class="p">(),</span> <span class="n">Adam</span><span class="p">())</span>
</pre></div>
</div>
<p>The QNN can be trained utilizing the <a class="reference internal" href="../modules/generated/squlearn.qnn.QNNRegressor.html#squlearn.qnn.QNNRegressor.fit" title="squlearn.qnn.QNNRegressor.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit</span></code></a> method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="c1"># Data that is inputted to the QNN</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="c1"># Data that is fitted by the QNN</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>

<span class="n">qnn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
<p>The inference of the QNN is calculated using the
<a class="reference internal" href="../modules/generated/squlearn.qnn.QNNRegressor.html#squlearn.qnn.QNNRegressor.predict" title="squlearn.qnn.QNNRegressor.predict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">predict</span></code></a> method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">qnn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="optimization">
<h2>Optimization<a class="headerlink" href="#optimization" title="Permalink to this heading"></a></h2>
<p>sQUlearn offers a lot of possibilities to train a QNN’s parameters. In this
section we will show, how to use <a class="reference internal" href="../modules/generated/squlearn.optimizers.SLSQP.html#squlearn.optimizers.SLSQP" title="squlearn.optimizers.optimizers_wrapper.SLSQP"><code class="xref py py-class docutils literal notranslate"><span class="pre">SLSQP</span></code></a>,
as an example for a wrapped scipy optimizer, and <a class="reference internal" href="../modules/generated/squlearn.optimizers.Adam.html#squlearn.optimizers.Adam" title="squlearn.optimizers.adam.Adam"><code class="xref py py-class docutils literal notranslate"><span class="pre">Adam</span></code></a>
with mini-batch gradient descent to optimize the loss function.</p>
<section id="slsqp">
<h3>SLSQP<a class="headerlink" href="#slsqp" title="Permalink to this heading"></a></h3>
<p>sQUlearn offers wrapper functions, <a class="reference internal" href="../modules/generated/squlearn.optimizers.SLSQP.html#squlearn.optimizers.SLSQP" title="squlearn.optimizers.optimizers_wrapper.SLSQP"><code class="xref py py-class docutils literal notranslate"><span class="pre">SLSQP</span></code></a>
and <a class="reference internal" href="../modules/generated/squlearn.optimizers.LBFGSB.html#squlearn.optimizers.LBFGSB" title="squlearn.optimizers.optimizers_wrapper.LBFGSB"><code class="xref py py-class docutils literal notranslate"><span class="pre">LBFGSB</span></code></a>, for scipy’s SLSQP and
L-BFGS-B implementations as well as the wrapper function
<a class="reference internal" href="../modules/generated/squlearn.optimizers.SPSA.html#squlearn.optimizers.SPSA" title="squlearn.optimizers.optimizers_wrapper.SPSA"><code class="xref py py-class docutils literal notranslate"><span class="pre">SPSA</span></code></a> for Qiskit’s SPSA implementation.
We show how to import and use <a class="reference internal" href="../modules/generated/squlearn.optimizers.SLSQP.html#squlearn.optimizers.SLSQP" title="squlearn.optimizers.optimizers_wrapper.SLSQP"><code class="xref py py-class docutils literal notranslate"><span class="pre">SLSQP</span></code></a>
in the following code block, other optimization methods can be used analogously.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">squlearn.optimizers</span> <span class="kn">import</span> <span class="n">SLSQP</span>

<span class="o">...</span>
<span class="n">slsqp</span> <span class="o">=</span> <span class="n">SLSQP</span><span class="p">(</span><span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;maxiter&quot;</span><span class="p">:</span> <span class="mi">100</span><span class="p">})</span>
<span class="o">...</span>
<span class="n">reg</span> <span class="o">=</span> <span class="n">QNNRegressor</span><span class="p">(</span>
    <span class="o">...</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">slsqp</span><span class="p">,</span>
    <span class="o">...</span>
<span class="p">)</span>
</pre></div>
</div>
<p>With this configuration, <a class="reference internal" href="../modules/generated/squlearn.qnn.QNNRegressor.html#squlearn.qnn.QNNRegressor" title="squlearn.qnn.QNNRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">QNNRegressor</span></code></a> will use scipy’s
<a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize" title="(in SciPy v1.11.3)"><code class="xref py py-func docutils literal notranslate"><span class="pre">minimize</span></code></a> function with <code class="docutils literal notranslate"><span class="pre">method=&quot;SLSQP&quot;</span></code>.
The wrapper Class <a class="reference internal" href="../modules/generated/squlearn.optimizers.SLSQP.html#squlearn.optimizers.SLSQP" title="squlearn.optimizers.optimizers_wrapper.SLSQP"><code class="xref py py-class docutils literal notranslate"><span class="pre">SLSQP</span></code></a>
allows to specify hyper parameters in a <code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code> that get passed on to the function.</p>
</section>
<section id="mini-batch-gradient-descent-with-adam">
<h3>Mini-Batch gradient descent with Adam<a class="headerlink" href="#mini-batch-gradient-descent-with-adam" title="Permalink to this heading"></a></h3>
<p>sQUlearn’s QNN classes, <a class="reference internal" href="../modules/generated/squlearn.qnn.QNNRegressor.html#squlearn.qnn.QNNRegressor" title="squlearn.qnn.QNNRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">QNNRegressor</span></code></a> and <a class="reference internal" href="../modules/generated/squlearn.qnn.QNNClassifier.html#squlearn.qnn.QNNClassifier" title="squlearn.qnn.QNNClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">QNNClassifier</span></code></a>, also offer the
possibility to use mini-batch gradient descent with Adam to optimize the model. This allows for
training on bigger data sets. Therefore we import and use the
<a class="reference internal" href="../modules/generated/squlearn.optimizers.Adam.html#squlearn.optimizers.Adam" title="squlearn.optimizers.adam.Adam"><code class="xref py py-class docutils literal notranslate"><span class="pre">Adam</span></code></a> optimizer as demonstrated in the following
code block.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">squlearn.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>

<span class="o">...</span>

<span class="n">adam</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">options</span><span class="o">=</span><span class="n">options_dict</span><span class="p">)</span>

<span class="o">...</span>

<span class="n">reg</span> <span class="o">=</span> <span class="n">QNNRegressor</span><span class="p">(</span>
    <span class="o">...</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">adam</span><span class="p">,</span>
    <span class="o">...</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="o">...</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Using SGD optimizers like the <a class="reference internal" href="../modules/generated/squlearn.optimizers.Adam.html#squlearn.optimizers.Adam" title="squlearn.optimizers.adam.Adam"><code class="xref py py-class docutils literal notranslate"><span class="pre">Adam</span></code></a> optimizer allows us
to specify further hyper parameters such as <code class="docutils literal notranslate"><span class="pre">batch_size</span></code>, <code class="docutils literal notranslate"><span class="pre">epochs</span></code> and <code class="docutils literal notranslate"><span class="pre">shuffle</span></code>.
The parameters <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> and <code class="docutils literal notranslate"><span class="pre">epochs</span></code> are positive numbers of type <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> and
<code class="docutils literal notranslate"><span class="pre">shuffle</span></code> is a <code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code> which specifies, whether data points are shuffled before each epoch.</p>
</section>
</section>
<section id="variance-reduction">
<h2>Variance reduction<a class="headerlink" href="#variance-reduction" title="Permalink to this heading"></a></h2>
<p>When evaluating a pretrained QNN on Qiskit’s <a class="reference external" href="https://qiskit.org/ecosystem/aer/stubs/qiskit_aer.QasmSimulator.html#qiskit_aer.QasmSimulator" title="(in Qiskit Aer v0.12.2)"><code class="xref py py-class docutils literal notranslate"><span class="pre">QasmSimulator</span></code></a> or
on real hardware, the model output will be subject to randomness due to the finite number of shots.
The noise level of the model thus depends on its variance, which can be calculated as</p>
<div class="math notranslate nohighlight">
\[\sigma_f^2 = \langle\Psi\lvert\hat{C}^2\rvert\Psi\rangle -
\langle\Psi\lvert\hat{C}\rvert\Psi\rangle^2 \text{.}\]</div>
<p><a class="reference internal" href="#figure-2">Figure 2</a> shows the output of a <a class="reference internal" href="../modules/generated/squlearn.qnn.QNNRegressor.html#squlearn.qnn.QNNRegressor" title="squlearn.qnn.QNNRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">QNNRegressor</span></code></a> fit to a logarithm
with <a class="reference internal" href="../modules/generated/squlearn.qnn.loss.SquaredLoss.html#squlearn.qnn.loss.SquaredLoss" title="squlearn.qnn.loss.SquaredLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">SquaredLoss</span></code></a> evaluated on Qiskit’s
<a class="reference external" href="https://qiskit.org/ecosystem/aer/stubs/qiskit_aer.QasmSimulator.html#qiskit_aer.QasmSimulator" title="(in Qiskit Aer v0.12.2)"><code class="xref py py-class docutils literal notranslate"><span class="pre">QasmSimulator</span></code></a>.
The model has been trained with a noise-free simulator, but evaluating it on a noisy simulator
yields a high variance in the model output.</p>
<p id="figure-2">(<a class="reference download internal" download="" href="../_downloads/9c7cb378db71dd36a9fcfe4d818f6320/quantum_neural_networks-2.py"><code class="xref download docutils literal notranslate"><span class="pre">Source</span> <span class="pre">code</span></code></a>, <a class="reference download internal" download="" href="../_downloads/56f2e61f3c6480b9e56a0b34522ebad9/quantum_neural_networks-2.png"><code class="xref download docutils literal notranslate"><span class="pre">png</span></code></a>, <a class="reference download internal" download="" href="../_downloads/1e8e97a87317428c7aa0aed975e04c66/quantum_neural_networks-2.hires.png"><code class="xref download docutils literal notranslate"><span class="pre">hires.png</span></code></a>, <a class="reference download internal" download="" href="../_downloads/b0f8fdf2b01e947525bdbb9acc43a777/quantum_neural_networks-2.pdf"><code class="xref download docutils literal notranslate"><span class="pre">pdf</span></code></a>)</p>
<figure class="align-center" id="id4">
<img alt="../_images/quantum_neural_networks-2.png" class="plot-directive" src="../_images/quantum_neural_networks-2.png" />
<figcaption>
<p><span class="caption-text"><strong>Figure 2</strong> Logarithm and output of <a class="reference internal" href="../modules/generated/squlearn.qnn.QNNRegressor.html#squlearn.qnn.QNNRegressor" title="squlearn.qnn.QNNRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">QNNRegressor</span></code></a> <span class="math notranslate nohighlight">\(f(\theta, x)\)</span> evaluated on Qiskit’s
<a class="reference external" href="https://qiskit.org/ecosystem/aer/stubs/qiskit_aer.QasmSimulator.html#qiskit_aer.QasmSimulator" title="(in Qiskit Aer v0.12.2)"><code class="xref py py-class docutils literal notranslate"><span class="pre">QasmSimulator</span></code></a>. The QNN output has a high variance.</span><a class="headerlink" href="#id4" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>We can mitigate this problem by adding the models variance to the loss function
<span class="math notranslate nohighlight">\(L_\text{fit}\)</span> and thus regularizing for variance. We do this by setting the <cite>variance</cite>
keyword in the initialization of the <a class="reference internal" href="../modules/generated/squlearn.qnn.QNNRegressor.html#squlearn.qnn.QNNRegressor" title="squlearn.qnn.QNNRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">QNNRegressor</span></code></a> (or <a class="reference internal" href="../modules/generated/squlearn.qnn.QNNClassifier.html#squlearn.qnn.QNNClassifier" title="squlearn.qnn.QNNClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">QNNClassifier</span></code></a>) with a
hyper-parameter <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">reg</span> <span class="o">=</span> <span class="n">QNNRegressor</span><span class="p">(</span>
    <span class="o">...</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="n">alpha</span><span class="p">,</span>
    <span class="o">...</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The new total loss function reads as</p>
<div class="math notranslate nohighlight">
\[L_\text{total} = L_\text{fit} +
\alpha \cdot \sum_k \lVert \sigma_f^2 ( x_i )\rVert \text{,}\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma_f^2( x_i )\)</span> is the variance of the QNN on the training data
<span class="math notranslate nohighlight">\(\{x_i\}\)</span>.</p>
<p>The regularization factor <span class="math notranslate nohighlight">\(\alpha\)</span> controls the influence of the variance regularization on
the total loss. It can be either set to a constant <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code> or a <code class="xref py py-class docutils literal notranslate"><span class="pre">Callable</span></code> that
takes the keyword argument <code class="docutils literal notranslate"><span class="pre">iteration</span></code> to dynamically adjust the factor. Values between
<span class="math notranslate nohighlight">\(10^{-2}\)</span> and <span class="math notranslate nohighlight">\(10^{-4}\)</span> have shown to yield satisfying results. <a class="reference internal" href="#id2">[1]</a></p>
<p>Evaluation on Qiskit’s <a class="reference external" href="https://qiskit.org/ecosystem/aer/stubs/qiskit_aer.QasmSimulator.html#qiskit_aer.QasmSimulator" title="(in Qiskit Aer v0.12.2)"><code class="xref py py-class docutils literal notranslate"><span class="pre">QasmSimulator</span></code></a> now yields less variance
in the model, as depicted in <a class="reference internal" href="#figure-3">figure 3</a>.</p>
<p id="figure-3">(<a class="reference download internal" download="" href="../_downloads/0c2b21d241e0543226e75390f9302f21/quantum_neural_networks-3.py"><code class="xref download docutils literal notranslate"><span class="pre">Source</span> <span class="pre">code</span></code></a>, <a class="reference download internal" download="" href="../_downloads/6d17bf658adef07c90841ef499d15734/quantum_neural_networks-3.png"><code class="xref download docutils literal notranslate"><span class="pre">png</span></code></a>, <a class="reference download internal" download="" href="../_downloads/be11cc17f34886436f374c0620515450/quantum_neural_networks-3.hires.png"><code class="xref download docutils literal notranslate"><span class="pre">hires.png</span></code></a>, <a class="reference download internal" download="" href="../_downloads/8bed8ceec30ed84b7a6ef68c1e5b6f80/quantum_neural_networks-3.pdf"><code class="xref download docutils literal notranslate"><span class="pre">pdf</span></code></a>)</p>
<figure class="align-center" id="id6">
<img alt="../_images/quantum_neural_networks-3.png" class="plot-directive" src="../_images/quantum_neural_networks-3.png" />
<figcaption>
<p><span class="caption-text"><strong>Figure 3</strong> Logarithm and output of <a class="reference internal" href="../modules/generated/squlearn.qnn.QNNRegressor.html#squlearn.qnn.QNNRegressor" title="squlearn.qnn.QNNRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">QNNRegressor</span></code></a> <span class="math notranslate nohighlight">\(f(\theta, x)\)</span>, trained with variance
regularization, evaluated on Qiskit’s <a class="reference external" href="https://qiskit.org/ecosystem/aer/stubs/qiskit_aer.QasmSimulator.html#qiskit_aer.QasmSimulator" title="(in Qiskit Aer v0.12.2)"><code class="xref py py-class docutils literal notranslate"><span class="pre">QasmSimulator</span></code></a>.
The QNN output has a low variance.</span><a class="headerlink" href="#id6" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p class="rubric">References</p>
<p><span class="target" id="id2">[1]</span> D. A. Kreplin and M. Roth “Reduction of finite sampling noise in quantum neural networks”.
<a class="reference external" href="https://arxiv.org/abs/2306.01639">arXiv:2306.01639</a> (2023).</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="kernel_methods.html" class="btn btn-neutral float-left" title="Quantum Kernel Methods" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../modules/classes.html" class="btn btn-neutral float-right" title="API Reference" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Fraunhofer IPA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>